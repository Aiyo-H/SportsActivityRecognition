{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f075d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all dependencies\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "from matplotlib.collections import LineCollection\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0770174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try camera capture\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "        \n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # display to screen\n",
    "        cv2.imshow('video capture', frame)\n",
    "            \n",
    "        # Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a3eb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Helper functions\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, fps):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, fps=fps)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a9aec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_name = \"movenet_lightning\"\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "  if \"movenet_lightning_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_f16\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  elif \"movenet_lightning_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder_int8\" in model_name:\n",
    "    !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  # Initialize the TFLite interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores\n",
    "\n",
    "else:\n",
    "  if \"movenet_lightning\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "    input_size = 192\n",
    "  elif \"movenet_thunder\" in model_name:\n",
    "    module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "    input_size = 256\n",
    "  else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "  def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    model = module.signatures['serving_default']\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = model(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a9a070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42549047 0.43278465 0.5444765  0.38204643 0.47446996 0.7259425\n",
      " 0.37924474 0.38446122 0.6296815  0.38354918 0.515577   0.56189644\n",
      " 0.38273415 0.31135556 0.70981675 0.48329577 0.5898636  0.51802987\n",
      " 0.51127815 0.22126776 0.6386699  0.68134236 0.69870603 0.54877305\n",
      " 0.746228   0.20930386 0.4969758  0.6067182  0.75045764 0.5188302\n",
      " 0.7583356  0.42308867 0.20144114 0.7807901  0.559992   0.40153325\n",
      " 0.7953875  0.34320718 0.36094886 0.72064316 0.70056343 0.10607925\n",
      " 0.73343766 0.28714418 0.11325663 0.7711828  0.4893267  0.16492221\n",
      " 0.744725   0.49592584 0.12548757]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the input image.\n",
    "im = cv2.imread('tt.jpg')\n",
    "\n",
    "input_image = tf.expand_dims(im, axis=0)\n",
    "input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "keypoints_with_scores = movenet(input_image)\n",
    "        # Run model inference.\n",
    "display_image = tf.expand_dims(im, axis=0)\n",
    "display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    display_image, 1280, 1280), dtype=tf.int32)\n",
    "output_overlay = draw_prediction_on_image(\n",
    "    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "print(keypoints_with_scores.flatten())\n",
    "cv2.imshow('img', output_overlay)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6c972c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_with_scores[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "780b5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "        prevTime = 0\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # display to screen\n",
    "        # Calculate and display\n",
    "        input_image = tf.expand_dims(frame, axis=0)\n",
    "        input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "        # Run model inference.\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "        # Visualize the predictions with image.\n",
    "        display_image = tf.expand_dims(frame, axis=0)\n",
    "        display_image = tf.cast(tf.image.resize_with_pad(\n",
    "            display_image, 1280, 1280), dtype=tf.int32)\n",
    "        output_overlay = draw_prediction_on_image(\n",
    "            np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "        currTime = time.time()\n",
    "        fps = 1 / (currTime - prevTime)\n",
    "        prevTime = currTime\n",
    "        #plt.figure(figsize=(5, 5))\n",
    "        cv2.putText(output_overlay, f'FPS: {int(fps)}', (20, 70), cv2.FONT_HERSHEY_PLAIN, 3, (0, 196, 255), 2)\n",
    "        cv2.imshow('img',output_overlay)\n",
    "        # Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da0394b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "c = np.array(keypoints_with_scores.flatten())\n",
    "print(type(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d092734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d630663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6559156 , 0.72443557, 0.4138219 , 0.6166811 , 0.7778954 ,\n",
       "       0.639906  , 0.588055  , 0.6641348 , 0.5117934 , 0.63595104,\n",
       "       0.80376625, 0.5820173 , 0.59039855, 0.5363361 , 0.46274075,\n",
       "       0.75294846, 0.8168471 , 0.52782726, 0.7618448 , 0.36229166,\n",
       "       0.48607445, 0.8864328 , 0.91721976, 0.23484388, 0.8864305 ,\n",
       "       0.26026797, 0.13810208, 0.8405983 , 0.8048035 , 0.05706173,\n",
       "       0.71531314, 0.5416248 , 0.10091946, 0.84768534, 0.49462116,\n",
       "       0.12050214, 0.84226525, 0.21473715, 0.07177103, 0.83141154,\n",
       "       0.7795328 , 0.05651763, 0.84563744, 0.28721422, 0.04540712,\n",
       "       0.8052138 , 0.6221494 , 0.07318342, 0.7024039 , 0.5272614 ,\n",
       "       0.07673904], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f30b00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.6559156 , 0.72443557, 0.4138219 ],\n",
       "         [0.6166811 , 0.7778954 , 0.639906  ],\n",
       "         [0.588055  , 0.6641348 , 0.5117934 ],\n",
       "         [0.63595104, 0.80376625, 0.5820173 ],\n",
       "         [0.59039855, 0.5363361 , 0.46274075],\n",
       "         [0.75294846, 0.8168471 , 0.52782726],\n",
       "         [0.7618448 , 0.36229166, 0.48607445],\n",
       "         [0.8864328 , 0.91721976, 0.23484388],\n",
       "         [0.8864305 , 0.26026797, 0.13810208],\n",
       "         [0.8405983 , 0.8048035 , 0.05706173],\n",
       "         [0.71531314, 0.5416248 , 0.10091946],\n",
       "         [0.84768534, 0.49462116, 0.12050214],\n",
       "         [0.84226525, 0.21473715, 0.07177103],\n",
       "         [0.83141154, 0.7795328 , 0.05651763],\n",
       "         [0.84563744, 0.28721422, 0.04540712],\n",
       "         [0.8052138 , 0.6221494 , 0.07318342],\n",
       "         [0.7024039 , 0.5272614 , 0.07673904]]]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a4acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for dataset\n",
    "DATA_PATH = os.path.join('dataset') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['jumping-jack', 'squat'])\n",
    "\n",
    "# 30 videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 62 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "for action in actions: \n",
    "    for sequence in range(no_sequences):\n",
    "        try: \n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bffb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.join('dataset') \n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['jumping-jack', 'squat'])\n",
    "\n",
    "# 30 videos worth of data\n",
    "no_sequences = 30\n",
    "\n",
    "# Videos are going to be 62 frames in length\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ad7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "action = \"jumping-jack\"\n",
    "if action == \"jumping-jack\": \n",
    "    for sequence in range(no_sequences):\n",
    "        for frame_num in range(sequence_length):\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            # display to screen\n",
    "            # Calculate and display\n",
    "            input_image = tf.expand_dims(frame, axis=0)\n",
    "            input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "            # Run model inference.\n",
    "            keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "            \n",
    "            # Visualize the predictions with image.\n",
    "            display_image = tf.expand_dims(frame, axis=0)\n",
    "            display_image = tf.cast(tf.image.resize_with_pad(\n",
    "                display_image, 1280, 1280), dtype=tf.int32)\n",
    "            output_overlay = draw_prediction_on_image(\n",
    "                np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "            if frame_num == 0: \n",
    "                cv2.putText(output_overlay, 'STARTING COLLECTION', (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', output_overlay)\n",
    "                cv2.waitKey(3000)\n",
    "            else: \n",
    "                cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', output_overlay)\n",
    "            keypoints = np.array(keypoints_with_scores.flatten())\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "            \n",
    "            # Break\n",
    "        \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5dfd958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For squat\n",
    "action = \"squat\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "if action == \"squat\": \n",
    "    for sequence in range(0,10):\n",
    "        for frame_num in range(20):\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            # display to screen\n",
    "            # Calculate and display\n",
    "            input_image = tf.expand_dims(frame, axis=0)\n",
    "            input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "            # Run model inference.\n",
    "            keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "            \n",
    "            # Visualize the predictions with image.\n",
    "            display_image = tf.expand_dims(frame, axis=0)\n",
    "            display_image = tf.cast(tf.image.resize_with_pad(\n",
    "                display_image, 1280, 1280), dtype=tf.int32)\n",
    "            output_overlay = draw_prediction_on_image(\n",
    "                np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "            if frame_num == 0: \n",
    "                cv2.putText(output_overlay, 'Pause for 5 secs', (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', output_overlay)\n",
    "                cv2.waitKey(5000)\n",
    "                cv2.putText(output_overlay, 'Start!', (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.waitKey(1000)\n",
    "            else: \n",
    "                cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', output_overlay)\n",
    "            keypoints = np.array(keypoints_with_scores.flatten())\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "            \n",
    "            # Break\n",
    "        \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02893e7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24036/1568097334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;31m# Show to screen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'OpenCV Feed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_overlay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m                 \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "action = \"x\"\n",
    "cap = cv2.VideoCapture(0)\n",
    "if action == \"x\": \n",
    "    for sequence in range(10,30):\n",
    "        for frame_num in range(sequence_length):\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "            # display to screen\n",
    "            # Calculate and display\n",
    "            input_image = tf.expand_dims(frame, axis=0)\n",
    "            input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "            # Run model inference.\n",
    "            keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "            \n",
    "            # Visualize the predictions with image.\n",
    "            display_image = tf.expand_dims(frame, axis=0)\n",
    "            display_image = tf.cast(tf.image.resize_with_pad(\n",
    "                display_image, 1280, 1280), dtype=tf.int32)\n",
    "            output_overlay = draw_prediction_on_image(\n",
    "                np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "            if frame_num == 0: \n",
    "                cv2.putText(output_overlay, 'STARTING COLLECTION', (120,200), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', output_overlay)\n",
    "                cv2.waitKey(3000)\n",
    "            else: \n",
    "                cv2.putText(output_overlay, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,12), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                # Show to screen\n",
    "                cv2.imshow('OpenCV Feed', output_overlay)\n",
    "            keypoints = np.array(keypoints_with_scores.flatten())\n",
    "            npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "            np.save(npy_path, keypoints)\n",
    "            \n",
    "            # Break\n",
    "        \n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed630a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d646dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild and load model\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "actions = np.array(['downward-dog','jumping-jack', 'leg-up', 'squat'])\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,51)))\n",
    "model1.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model1.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model1.add(Dense(64, activation='relu'))\n",
    "model1.add(Dense(32, activation='relu'))\n",
    "model1.add(Dense(actions.shape[0], activation='softmax'))\n",
    "\n",
    "model1.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7feddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_weights('djls.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baf959e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245), (255,0,0)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (128, 0, 128), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "250b599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "squat\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n",
      "jumping-jack\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.95\n",
    "while cap.isOpened():\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "        # display to screen\n",
    "        # Calculate and display\n",
    "        input_image = tf.expand_dims(frame, axis=0)\n",
    "        input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "        # Run model inference.\n",
    "        keypoints_with_scores = movenet(input_image)\n",
    "\n",
    "        # Visualize the predictions with image.\n",
    "        display_image = tf.expand_dims(frame, axis=0)\n",
    "        display_image = tf.cast(tf.image.resize_with_pad(\n",
    "            display_image, 1280, 1280), dtype=tf.int32)\n",
    "        output_overlay = draw_prediction_on_image(\n",
    "            np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "        # Prediction\n",
    "        keypoints = np.array(keypoints_with_scores.flatten())\n",
    "        sequence.insert(0,keypoints)\n",
    "        sequence = sequence[:30]\n",
    "        \n",
    "        if len(sequence) == 30:\n",
    "            res = model1.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "            print(actions[np.argmax(res)])\n",
    "        \n",
    "            # Viz\n",
    "            if res[np.argmax(res)] > threshold:\n",
    "                if len(sentence) > 0:\n",
    "                    if actions[np.argmax(res)] != sentence[-1]:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "            if len(sentence) > 5:\n",
    "                sentence = sentence[-5:]\n",
    "            output_overlay = prob_viz(res, actions, output_overlay, colors)\n",
    "        \n",
    "        cv2.rectangle(output_overlay, (0,0), (1280, 40), (245, 117, 16), -1)\n",
    "        cv2.putText(output_overlay, ' '.join(sentence), (3,30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('img',output_overlay)\n",
    "        # Break\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4b3ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2bf22ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(output_overlay, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "cv2.imshow('img',output_overlay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e8dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
